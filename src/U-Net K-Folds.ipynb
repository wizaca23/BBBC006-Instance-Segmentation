{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"U-Net K-Folds.ipynb","provenance":[{"file_id":"1Xte-8-XdNNsDeksKJcIMbuysNOZbRc3f","timestamp":1572749352818}],"collapsed_sections":["XZ68m-0p5kLc","goV5UCuSAe6I","g2-mVhXu4xJB","JNIr071X66-b","42EaSOf5Ra-V","CjHAPiB07FZp","ICIBtQkkBcHV","Z2anqKpfBiCl","pw6yUiOCBo6o","3MFjSGs77YTh","8sNbQ4IW7yCm","gY4F4XEwJGxC","QiKZWlCfJUeS","GXPoes2MTVej","4zOmy7-L8A84","jsUC0bMoIarz","ichvI4LqIfg-","BMGqVF558hyO","2rarvm04UjoE","oNJOhMjpUqSH","Ec_ySByt5Kql","Q3_TfdG5bLiF"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XZ68m-0p5kLc","colab_type":"text"},"source":["# Setup the Running Drive Folder\n","\n"]},{"cell_type":"code","metadata":{"id":"nk0yzwdS6YSo","colab_type":"code","colab":{}},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# Mount the drive folder. This will prompt for authorization.\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Opens the project folder. IMPORTANT: Change to your route\n","%cd 'drive/My Drive/TEC/Tesis/BBBC006x256/'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"goV5UCuSAe6I","colab_type":"text"},"source":["# Dependencies and Libraries\n","\n"]},{"cell_type":"markdown","metadata":{"id":"g2-mVhXu4xJB","colab_type":"text"},"source":["## Install Dependencies\n","\n"]},{"cell_type":"code","metadata":{"id":"YXWLX1mu5SV5","colab_type":"code","outputId":"c8c18610-c36b-422c-c84e-1408b2175643","executionInfo":{"status":"ok","timestamp":1573544022838,"user_tz":360,"elapsed":7708,"user":{"displayName":"Willard Zamora CÃ¡rdenas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mD_PVWaPCeonLrpgLVmHqRfwMWgKxA_zrXxxzDnEQ=s64","userId":"16851418437110843234"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["!pip install scipy\n","!pip install torchvision"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.3.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.17.3)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.2+cu100)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.3)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n","Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.3.1+cu100)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JNIr071X66-b","colab_type":"text"},"source":["## Modules Import"]},{"cell_type":"code","metadata":{"id":"J6Y6my8a7Bf_","colab_type":"code","colab":{}},"source":["import sys\n","import os\n","import numpy as np\n","#import cupy as cp\n","import random\n","import time\n","import math\n","import csv\n","import json\n","import pandas as pd\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import optim\n","from torch import argmax\n","from torch.utils.data.dataset import Dataset\n","from torch.utils.data.dataset import ConcatDataset\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.utils import save_image\n","from torchvision.utils import make_grid\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torchvision.transforms.functional as Ft\n","from torch.autograd.variable import Variable\n","\n","from scipy.ndimage.morphology import distance_transform_edt as dist_trans"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"42EaSOf5Ra-V","colab_type":"text"},"source":["## Global Variables\n","\n","Configure global variables such as the mean and std of the dataset you are going to use."]},{"cell_type":"code","metadata":{"id":"rTyqE53gRfaN","colab_type":"code","colab":{}},"source":["#For Original Dataset\n","globalMean = 0.0054;\n","globalStd = 0.0037;"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CjHAPiB07FZp","colab_type":"text"},"source":["# Misc Functions\n","\n","Auxiliary code, used to save the network status, results and predictions"]},{"cell_type":"markdown","metadata":{"id":"ICIBtQkkBcHV","colab_type":"text"},"source":["## Save Network Epoch Results\n","\n"]},{"cell_type":"code","metadata":{"id":"4ZMKzkup7Lm_","colab_type":"code","colab":{}},"source":["\"\"\" \n","    Export data to csv format. Creates new file if doesn't exist,\n","    otherwise update it.\n","    Args:\n","        header (list): headers of the column\n","        value (list): values of correspoding column\n","        folder: folder path\n","        file_name: file name with path\n","\"\"\"\n","def export_history(header, value, folder, file_name):\n","    # If folder does not exists make folder\n","    if not os.path.exists(folder):\n","        os.makedirs(folder)\n","\n","    file_path = folder + file_name\n","    file_existence = os.path.isfile(file_path)\n","\n","    # If there is no file make file\n","    if file_existence == False:\n","        file = open(file_path, 'w', newline='')\n","        writer = csv.writer(file)\n","        writer.writerow(header)\n","        writer.writerow(value)\n","    # If there is file overwrite\n","    else:\n","        file = open(file_path, 'a', newline='')\n","        writer = csv.writer(file)\n","        writer.writerow(value)\n","    # Close file when it is done with writing\n","    file.close()\n","\n","\n","\"\"\" \n","    Computes and stores the average and current value\n","    Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n","\"\"\"  \n","class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z2anqKpfBiCl","colab_type":"text"},"source":["## Save Network State"]},{"cell_type":"code","metadata":{"id":"6A7cuPguBmeO","colab_type":"code","colab":{}},"source":["\"\"\" \n","    Save the state of a net.\n","\"\"\"\n","def save_checkpoint(state, path='checkpoint/', filename='weights.pth'):\n","    # If folder does not exists make folder\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","    filepath = os.path.join(path, filename)\n","    torch.save(state, filepath)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pw6yUiOCBo6o","colab_type":"text"},"source":["## Save Network Predictions"]},{"cell_type":"code","metadata":{"id":"TALcAl2YCAmd","colab_type":"code","colab":{}},"source":["def to_image(tensor, nrow=8, padding=2,\n","               normalize=False, range=None, scale_each=False, pad_value=0):\n","    \"\"\"Save a given Tensor into an image file.\n","\n","    Args:\n","        tensor (Tensor or list): Image to be saved. If given a mini-batch tensor,\n","            saves the tensor as a grid of images by calling ``make_grid``.\n","        **kwargs: Other arguments are documented in ``make_grid``.\n","    \"\"\"\n","    from PIL import Image\n","    tensor = tensor.cpu()\n","    grid = make_grid(tensor, nrow=nrow, padding=padding, pad_value=pad_value,\n","                     normalize=normalize, range=range, scale_each=scale_each)\n","    ndarr = grid.mul(255).clamp(0, 255).byte().permute(1, 2, 0).numpy()\n","    im = Image.fromarray(ndarr)\n","    return im"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lIi2QhOXCR2H","colab_type":"code","colab":{}},"source":["def save_dataset_loaders(dir_save, load, n_channels, n_classes, loaders, conversion, out, full_loader=True):\n","  with torch.no_grad():\n","\n","        print(\"\"\"\n","        \\nModel predictions are going to be saved with:\n","        Output Directory: {},\n","        Weights Directory: {},\n","        Number of Input Channels: {},\n","        Number of Classes: {},\n","        Conversion type: {},\n","        Out Function: {}\n","              \"\"\".format(dir_save, load, n_channels, n_classes, conversion, out))\n","        \n","        # Make sure that loaders is a list of loaders, if it is a single dataloader; pack into a list\n","        if isinstance(loaders, DataLoader):\n","          loaders = [loaders]\n","\n","        # Use GPU or not\n","        use_cuda = torch.cuda.is_available()\n","        device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","        # Create the model - 256 classes are used so the resulting image can better reflect the grayscale groundtruth\n","        #net = UNet(n_channels=1, n_classes=1).to(device)\n","        # Create the model\n","        net = UNet(n_channels=n_channels, n_classes= n_classes).to(device)\n","        net = torch.nn.DataParallel(net, device_ids=list(\n","            range(torch.cuda.device_count()))).to(device)\n","\n","        # Load old weights\n","        if load:\n","            net.load_state_dict(torch.load(load)[\"state_dict\"])\n","            print('Model loaded from {}'.format(load))\n","            \n","        net.eval()\n","        \n","        # Full loader is a variable that basically works to check the type of loader we are receiving in this function\n","        # is true if the loader is the same as the one used in training (img, gt, id) or false; specially made to save predictions (img, id)\n","        # This is necessary as we already had a function that saved the dataset with a special loader.\n","        if full_loader:\n","          \n","          for loader in loaders:\n","\n","            for batch_idx, (data, gt, id_n) in enumerate(loader):\n","   \n","              # Use GPU or not\n","              data = data.to(device, dtype=torch.float)\n","              \n","              # Forward\n","              predictions = net(data)\n","              \n","              if out == \"Softmax\":\n","                predictions = F.softmax(predictions)\n","              \n","              # Saves the image\n","              img = to_image(predictions)\n","              img_new = img.convert(conversion)\n","              img_new.save(dir_save + id_n[0])\n","\n","              if batch_idx % 10 == 0:\n","                print('[{}/{} ({:.0f}%)]'.format(\n","                  batch_idx, len(loader), 100. * batch_idx / len(loader)))\n","            \n","        else:\n","        \n","          for loader in loaders:\n","\n","            for batch_idx, (data, id_n) in enumerate(loader):\n","        \n","              # Use GPU or not\n","              data = data.to(device, dtype=torch.float)\n","              \n","              # Forward\n","              predictions = net(data)\n","              \n","              if out == \"Softmax\":\n","                predictions = F.softmax(predictions)\n","              \n","              # Saves the image\n","              img = to_image(predictions)\n","              img_new = img.convert(conversion)\n","              img_new.save(dir_save + id_n[0]) \n","\n","              if batch_idx % 10 == 0:\n","                print('[{}/{} ({:.0f}%)]'.format(\n","                  batch_idx, len(loader), 100. * batch_idx / len(loader)))\n","        \n","\n","def save_dataset(dir_save, load, n_channels, n_classes, dir_img, conversion, out, back_load=None, two_inputs=None):\n","  loader = get_dataloader_transform(dir_img, back_load, two_inputs)\n","  save_dataset_loaders(dir_save, load, n_channels, n_classes, loader, conversion, out, False)\n","  \n","def save_dataset_k_fold(dir_save, load, n_channels, n_classes, dir_img, conversion, out, k, dir_gt, batch_size, current_iter, back_model_load=None, two_inputs=None):\n","  ids = get_partitions(k)\n","  back_loaders = get_loaders_kfolds(ids, k, dir_img, dir_gt, batch_size, current_iter, back_model_load, two_inputs)\n","  save_dataset_loaders(dir_save, load, n_channels, n_classes, back_loaders[1], conversion, out, True)    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DWGxQad5Ws4Q","colab_type":"code","colab":{}},"source":["def save_predictions(i, k, dir_predictions, results_top, results_back, dir_img, dir_gt, two_inputs):\n","\n","  # Creates predictions' directory\n","  if not os.path.isdir(dir_predictions + \"Fold-{}/\".format(str(i))):\n","    os.makedirs(dir_predictions + \"Fold-{}/\".format(str(i)))\n","\n","  if results_back is not None:\n","    back_load = results_back + \"best_weights\" + str(i) + \".pth\"\n","  else:\n","    back_load = None\n","    two_inputs = None\n","\n","  if two_inputs:\n","    top_n_channels = 2\n","  else:\n","    top_n_channels = 1\n","    \n","  # Saves the results for future metrics calculation\n","  save_dataset_k_fold(dir_save=dir_predictions + \"Fold-{}/\".format(str(i)), \n","                load=results_top + \"best_weights\" + str(i) + \".pth\",\n","                n_channels = top_n_channels, \n","                n_classes=3, \n","                dir_img=dir_img, \n","                conversion='RGB', \n","                out=None,\n","                k=k,\n","                dir_gt=dir_gt,\n","                batch_size = 1,\n","                current_iter = i,\n","                back_model_load=back_load,\n","                two_inputs=two_inputs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3MFjSGs77YTh","colab_type":"text"},"source":["# U-Net Architecture\n","\n"]},{"cell_type":"code","metadata":{"id":"gS3XnHNX7kaw","colab_type":"code","colab":{}},"source":["\"\"\" \n","    This file defines every layer (or group of layers) that are inside UNet.\n","    At the final the architecture UNet is defined as a conjuntion of the elements created.\n","\"\"\"\n","class double_conv(nn.Module):\n","    ''' Applies (conv => BN => ReLU) two times. '''\n","\n","    def __init__(self, in_ch, out_ch):\n","        super(double_conv, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_ch),\n","            # inplace is for aply ReLU to the original place, saving memory\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_ch),\n","            # inplace is for aply ReLU to the original place, saving memory\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","\n","class inconv(nn.Module):\n","    ''' First Section of U-Net. '''\n","\n","    def __init__(self, in_ch, out_ch):\n","        super(inconv, self).__init__()\n","\n","        self.conv = double_conv(in_ch, out_ch)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","\n","class down(nn.Module):\n","    ''' Applies a MaxPool with a Kernel of 2x2,\n","        then applies a double convolution pack. '''\n","\n","    def __init__(self, in_ch, out_ch):\n","        super(down, self).__init__()\n","\n","        self.mpconv = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=2),\n","            double_conv(in_ch, out_ch)\n","        )\n","\n","    def forward(self, x):\n","        x = self.mpconv(x)\n","        return x\n","\n","\n","class up(nn.Module):\n","    ''' Applies a Deconvolution and then applies applies a double convolution pack. '''\n","\n","    def __init__(self, in_ch, out_ch, bilinear=False):\n","        super(up, self).__init__()\n","        \n","        # Bilinear is used to save computational cost\n","        if bilinear:\n","            self.up = nn.Upsample(\n","                scale_factor=2, mode='bilinear', align_corners=True)\n","        else:\n","            self.up = nn.ConvTranspose2d(\n","                in_ch//2, in_ch//2, kernel_size=2, stride=2)\n","\n","        self.conv = double_conv(in_ch, out_ch)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","        diffX = x1.size()[2] - x2.size()[2]\n","        diffY = x1.size()[3] - x2.size()[3]\n","        x2 = F.pad(input=x2, pad=(diffX // 2, diffX // 2,\n","                                  diffY // 2, diffY // 2))\n","        x = torch.cat([x2, x1], dim=1)\n","        x = self.conv(x)\n","        return x\n","\n","\n","class outconv(nn.Module):\n","    ''' Applies the last Convolution to give an answer. '''\n","\n","    def __init__(self, in_ch, out_ch):\n","        super(outconv, self).__init__()\n","\n","        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=1)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","      "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"INLK6iM87i70","colab_type":"code","colab":{}},"source":["class UNet(nn.Module):\n","    ''' This Object defines the architecture of U-Net. '''\n","\n","    def __init__(self, n_channels, n_classes):\n","        super(UNet, self).__init__()\n","\n","        self.inc = inconv(n_channels, 64)\n","        self.down1 = down(64, 128)\n","        self.down2 = down(128, 256)\n","        self.down3 = down(256, 512)\n","        self.down4 = down(512, 512)\n","        self.up1 = up(1024, 256)\n","        self.up2 = up(512, 128)\n","        self.up3 = up(256, 64)\n","        self.up4 = up(128, 64)\n","        self.outc = outconv(64, n_classes)\n","        \n","        \n","    def forward(self, x):\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x5 = self.down4(x4)\n","        x = self.up1(x5, x4)\n","        x = self.up2(x, x3)\n","        x = self.up3(x, x2)\n","        x = self.up4(x, x1)\n","        x = self.outc(x)\n","        #x = F.softmax(x) # New softmax layer\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8sNbQ4IW7yCm","colab_type":"text"},"source":["# Dataset Handlders\n","\n","The nexts cells are there for the loading process. In this case we use the Dataset and Dataloader objects given by Pytorch. In the training process we will call this section"]},{"cell_type":"markdown","metadata":{"id":"gY4F4XEwJGxC","colab_type":"text"},"source":["## Datasets\n","\n","In these cells we define the way our datasets will behave and how the data will be retrieved"]},{"cell_type":"code","metadata":{"id":"FCVc2fVig_Mk","colab_type":"code","colab":{}},"source":["'''\n","    Class that defines the reading and processing of the images.\n","    Specialized on BBBC006 dataset.\n","'''\n","class BBBCDataset(Dataset):\n","    def __init__(self, ids, dir_data, dir_gt, extension='.png'):\n","\n","        self.dir_data = dir_data\n","        self.dir_gt = dir_gt\n","        self.extension = extension\n","\n","        # Use GPU or not\n","        use_cuda = torch.cuda.is_available()\n","        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","        # Transforms\n","        self.data_transforms = {\n","            'imgs': transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Normalize([globalMean],[globalStd])\n","            ]),\n","            'masks': transforms.Compose([\n","                transforms.ToTensor()\n","            ]),\n","        }\n","\n","        # Images IDS\n","        self.ids = ids\n","\n","        # Calculate len of data\n","        self.data_len = len(self.ids)\n","\n","    '''\n","        Ask for an image.\n","    '''\n","    def __getitem__(self, index):\n","        # Get an ID of a specific image\n","        id_img = self.dir_data + self.ids[index] + self.extension\n","        id_gt = self.dir_gt + self.ids[index] + self.extension\n","        # Open Image and GroundTruth\n","        img = Image.open(id_img).convert('L')\n","        gt = Image.open(id_gt)\n","        # Applies transformations\n","        img = self.data_transforms['imgs'](img)\n","        img = img.to(self.device)\n","#        img = img / torch.max(img)\n","        gt = self.data_transforms['masks'](gt)\n","        gt = gt.to(self.device)\n","\n","        return (img, gt, self.ids[index]+self.extension)\n","\n","    '''\n","        Length of the dataset.\n","        It's needed for the upper class.\n","    '''\n","    def __len__(self):\n","        return self.data_len\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Mxv5M3gGyk2","colab_type":"code","colab":{}},"source":["'''\n","    Class that defines the reading and processing of the images.\n","    Specialized on BBBC006 dataset.\n","'''\n","class BBBCDataset_Top(Dataset):\n","    def __init__(self, ids, dir_data, dir_gt, load, extension='.png'):\n","\n","        self.dir_data = dir_data\n","        self.dir_gt = dir_gt\n","        self.extension = extension\n","        self.load = load\n","\n","        # Use GPU or not\n","        use_cuda = torch.cuda.is_available()\n","        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","        # Create the model\n","        # As this is the top model we know it will always have 1 channel of input and 1 class of output\n","        back_model = UNet(1, 1).to(self.device)\n","        self.back_model = torch.nn.DataParallel(back_model, device_ids=list(\n","        range(torch.cuda.device_count()))).to(self.device)\n","\n","        #self.back_model.eval()\n","\n","        # Load the weights\n","        self.back_model.load_state_dict(torch.load(load)['state_dict'])\n","        print('Model loaded from {}'.format(load))\n","\n","        # Transforms\n","        self.data_transforms = {\n","            'imgs': transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Normalize([globalMean],[globalStd])\n","            ]),\n","            'masks': transforms.Compose([\n","                transforms.ToTensor()\n","            ]),\n","            'preds': transforms.Compose([\n","                transforms.ToTensor()\n","            ]),\n","        }\n","\n","        # Images IDS\n","        self.ids = ids\n","\n","        # Calculate len of data\n","        self.data_len = len(self.ids)\n","\n","    '''\n","        Ask for an image.\n","    '''\n","    def __getitem__(self, index):\n","\n","        # Get an ID of a specific image\n","        id_img = self.dir_data + self.ids[index] + self.extension\n","        id_gt = self.dir_gt + self.ids[index] + self.extension\n","        # Open Image and GroundTruth\n","        img = Image.open(id_img).convert('L')\n","        gt = Image.open(id_gt)\n","        # Applies transformations\n","        img = self.data_transforms['imgs'](img)\n","        img = img.to(self.device)\n","        # Resize image to \"batch\" size so the weights work\n","        img = img[None,:,:,:]\n","#       img = img / torch.max(img)\n","        gt = self.data_transforms['masks'](gt)\n","        gt = gt.to(self.device)\n","        \n","        # Obtains the back_model prediction\n","        img_top = self.back_model(img)\n","        img_top = img_top[0,:,:,:]\n","        \n","        # Prediction to Greyscale\n","        #img_top = to_image(img_top)\n","        #img_top = img_top.convert('L')\n","\n","        #img_top = self.data_transforms['preds'](img_top)\n","        img_top = img_top.to(self.device)\n","        #img_top = img_top / 255;\n","\n","        return (img_top, gt, self.ids[index]+self.extension)\n","\n","    '''\n","        Length of the dataset.\n","        It's needed for the upper class.\n","    '''\n","    def __len__(self):\n","        return self.data_len\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MEbUP0s8Y1o0","colab_type":"code","colab":{}},"source":["'''\n","    Class that defines the reading and processing of the images.\n","    Specialized on BBBC006 dataset.\n","'''\n","class BBBCDataset_TwoInputs(Dataset):\n","    def __init__(self, ids, dir_data, dir_gt, load, extension='.png'):\n","\n","        self.dir_data = dir_data\n","        self.dir_gt = dir_gt\n","        self.extension = extension\n","        self.load = load\n","\n","        # Use GPU or not\n","        use_cuda = torch.cuda.is_available()\n","        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","        # Create the model\n","        # As this is the top model we know it will always have 1 channel of input and 1 class of output\n","        back_model = UNet(1, 1).to(self.device)\n","        self.back_model = torch.nn.DataParallel(back_model, device_ids=list(\n","        range(torch.cuda.device_count()))).to(self.device)\n","\n","        #self.back_model.eval()\n","\n","        # Load the weights\n","        self.back_model.load_state_dict(torch.load(load)['state_dict'])\n","        print('Model loaded from {}'.format(load))\n","\n","        # Transforms\n","        self.data_transforms = {\n","            'imgs': transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Normalize([globalMean],[globalStd])\n","            ]),\n","            'masks': transforms.Compose([\n","                transforms.ToTensor()\n","            ]),\n","            'preds': transforms.Compose([\n","                transforms.ToTensor()\n","            ]),\n","        }\n","\n","        # Images IDS\n","        self.ids = ids\n","\n","        # Calculate len of data\n","        self.data_len = len(self.ids)\n","\n","    '''\n","        Ask for an image.\n","    '''\n","    def __getitem__(self, index):\n","\n","        # Get an ID of a specific image\n","        id_img = self.dir_data + self.ids[index] + self.extension\n","        id_gt = self.dir_gt + self.ids[index] + self.extension\n","        # Open Image and GroundTruth\n","        img = Image.open(id_img).convert('L')\n","        gt = Image.open(id_gt)\n","        # Applies transformations\n","        img = self.data_transforms['imgs'](img)\n","        img = img.to(self.device)\n","        # Resize image to \"batch\" size so the weights work\n","        img_back = img[None,:,:,:]\n","#       img = img / torch.max(img)\n","        gt = self.data_transforms['masks'](gt)\n","        gt = gt.to(self.device)\n","        \n","        # Obtains the back_model prediction\n","        img_top = self.back_model(img_back)\n","        img_top = img_top[0,:,:,:]\n","        \n","        # Prediction to Greyscale\n","        #img_top = to_image(img_top)\n","        #img_top = img_top.convert('L')\n","\n","        #img_top = self.data_transforms['preds'](img_top)\n","        #img_top = img_top.to(self.device)\n","        #img_top = img_top / 255;\n","\n","        img_final = torch.cat((img,img_top),0)\n","        #img_final = img_final.to(self.device)\n","\n","        return (img_final, gt, self.ids[index]+self.extension)\n","\n","    '''\n","        Length of the dataset.\n","        It's needed for the upper class.\n","    '''\n","    def __len__(self):\n","        return self.data_len"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QiKZWlCfJUeS","colab_type":"text"},"source":["## Dataloaders\n","\n","In the next code cells we define some dataloaders to pull the data from the Dataset classes"]},{"cell_type":"code","metadata":{"id":"7uhCw8gK76wf","colab_type":"code","colab":{}},"source":["# We assume the existance of a file kfolds_partitions.txt with each partition\n","def get_partitions(k):\n","\n","  with open(\"kfolds_partitions.txt\") as file_partitions:\n","    \n","    # Read the partitions as JSON\n","    json_file = json.load(file_partitions)\n","\n","    ids_partitions = []\n","\n","    for i in range(0, k):\n","      \n","      partition_i = []\n","\n","      for id_val in json_file[\"partition_\" + str(i)]:\n","        partition_i.append(str(id_val))\n","\n","      ids_partitions.append(partition_i)\n","\n","    return ids_partitions\n","    \n","  \n","# Function in charge of obtaining the training and validation loaders from \n","# the ids partitioned with get_partitions\n","def get_loaders_kfolds(ids_partitions, k, dir_img, dir_gt, batch_size, current_iter, back_model_load, two_inputs):\n","  \n","  # Variable that holds the train partitions ids\n","  train_ids = []\n","\n","  # Iterates over the ids, while the ids_set examined is not the validation one, then it is appended to the train ids\n","  for j in range(0, k):\n","    if j != current_iter:\n","      \n","      train_ids += ids_partitions[j]\n","\n","  # Gets the datasets according to the kind of model we are training\n","  if back_model_load and two_inputs:\n","    val_dset = BBBCDataset_TwoInputs(ids=ids_partitions[current_iter], dir_data=dir_img, dir_gt=dir_gt, load=back_model_load)\n","    train_dset = BBBCDataset_TwoInputs(ids=train_ids, dir_data=dir_img, dir_gt=dir_gt, load=back_model_load)\n","  elif back_model_load:\n","    val_dset = BBBCDataset_Top(ids=ids_partitions[current_iter], dir_data=dir_img, dir_gt=dir_gt, load=back_model_load)\n","    train_dset = BBBCDataset_Top(ids=train_ids, dir_data=dir_img, dir_gt=dir_gt, load=back_model_load)\n","  else:\n","    val_dset = BBBCDataset(ids=ids_partitions[current_iter], dir_data=dir_img, dir_gt=dir_gt)\n","    train_dset = BBBCDataset(ids=train_ids, dir_data=dir_img, dir_gt=dir_gt)\n","\n","  # Obtains the array of loaders we need to wrap both our datasets\n","  loaders = [\n","             DataLoader(train_dset, batch_size=batch_size, shuffle=True),\n","             DataLoader(val_dset, batch_size=batch_size, shuffle=True)\n","  ]\n","\n","  return loaders"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GXPoes2MTVej","colab_type":"text"},"source":["## Dataset Mean-Std"]},{"cell_type":"code","metadata":{"id":"5gY4oITjTbeo","colab_type":"code","colab":{}},"source":["# Remember that the dataloader should have load without normalization\n","def getMeanStd(dataloader):\n","  # Use GPU or not\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","  (std, mean) = (0, 0)\n","  for batch_idx, (data, gt, weights) in enumerate(dataloader):\n","    # Use GPU or not\n","    data, gt = data.to(device), gt.to(device)\n","    (tempStd, tempMean) = torch.std_mean(data)\n","    std = std + tempStd\n","    mean = mean + tempMean\n","  std = std / len(dataloader)\n","  mean = mean / len(dataloader)\n","  return (mean, std)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4zOmy7-L8A84","colab_type":"text"},"source":["# Model Training & Validation"]},{"cell_type":"markdown","metadata":{"id":"jsUC0bMoIarz","colab_type":"text"},"source":["## Loss functions"]},{"cell_type":"code","metadata":{"id":"b2oyt2y2OP8e","colab_type":"code","colab":{}},"source":["\"\"\"\n","    Class that defines the Cross Entropy Loss Function\n","\"\"\"\n","class CELoss(nn.Module):\n","    def __init__(self):\n","        super(CELoss, self).__init__()\n","\n","    def forward(self, y_pred, y_true):\n","        return -torch.mean(torch.sum(y_true*torch.log(F.softmax(y_pred,dim=1)),dim=1))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ichvI4LqIfg-","colab_type":"text"},"source":["## Training & Validation Setup"]},{"cell_type":"code","metadata":{"id":"pB2zVEwN8IPa","colab_type":"code","colab":{}},"source":["\"\"\" \n","    Functions that trains a net.\n","\"\"\"\n","def train_net(net, device, loader, optimizer, criterion, batch_size, loss_type):\n","    \n","    net.train()\n","    train_loss = AverageMeter()\n","    train_acc = AverageMeter()\n","    time_start = time.time()\n","\n","    for batch_idx, (data, gt, weights) in enumerate(loader):\n","\n","        # Use GPU or not\n","        data, gt = data.to(device), gt.to(device)\n","\n","        # Forward\n","        predictions = net(data)\n","        \n","        # Loss Calculation\n","        if loss_type == \"Cross\":\n","            labels = argmax(gt, dim=1)\n","            loss = criterion(predictions, labels)\n","        else:\n","            loss = criterion(predictions, gt)\n","            \n","        # Updates the record\n","        train_loss.update(loss.item(), predictions.size(0))\n","        train_acc.update(-loss.item(), predictions.size(0))\n","        \n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","\n","        # Calculates new parameters\n","        optimizer.step()\n","\n","        print('[{}/{} ({:.0f}%)]\\t\\tLoss: {:.6f}'.format(\n","            batch_idx * len(data), len(loader)*batch_size,\n","            100. * batch_idx / len(loader), loss.item()))\n","\n","    time_dif = time.time() - time_start\n","    print('\\nAverage Training Loss: ' + str(train_loss.avg))\n","    print('\\nAverage Training Accuracy: ' + str(train_acc.avg))\n","    print('Train Time: It tooks %.4fs to finish the epoch.' % (time_dif))\n","            \n","    return train_loss.avg, train_acc.avg"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oEuOs9AD8Mp-","colab_type":"code","colab":{}},"source":["\"\"\" \n","    Function that validates the net.\n","\"\"\"\n","def val_net(net, device, loader, criterion, batch_size):\n","    net.eval()\n","    val_loss = AverageMeter()\n","    val_acc = AverageMeter()\n","    time_start = time.time()\n","    with torch.no_grad():\n","        for batch_idx, (data, gt, weights) in enumerate(loader):\n","\n","            # Use GPU or not\n","            data, gt = data.to(device), gt.to(device)\n","\n","            # Forward\n","            predictions = net(data)\n","            \n","            # Loss Calculation\n","            loss = criterion(predictions, gt)\n","\n","            # Updates the record\n","            val_loss.update(loss.item(), predictions.size(0))\n","            val_acc.update(-loss.item(), predictions.size(0))\n","            \n","            print('[{}/{} ({:.0f}%)]\\t\\tLoss: {:.6f}'.format(\n","                batch_idx * len(data), len(loader)*batch_size,\n","                100. * batch_idx / len(loader), loss.item()))\n","    \n","    time_dif = time.time() - time_start\n","    print('\\nValidation set: Average loss: '+ str(val_loss.avg))\n","    print('\\nAverage Validation Accuracy: ' + str(val_acc.avg))\n","    print('Validation time: It tooks %.4fs to finish the Validation.' % (time_dif))\n","    \n","    return val_loss.avg, val_acc.avg"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0X6dcfJN8PV6","colab_type":"code","colab":{}},"source":["'''\n","    Configure every aspect of the run.\n","    Runs the training and validation.\n","'''\n","def setup_and_run_train(n_channels, n_classes, dir_img, dir_gt, dir_results, load, \n","                val_perc, batch_size, epoch_inicial, epochs, lr, run, optimizer, loss, loaders):\n","    \n","    # Use GPU or not\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    # Create the model\n","    net = UNet(n_channels, n_classes).to(device)\n","    net = torch.nn.DataParallel(net, device_ids=list(\n","        range(torch.cuda.device_count()))).to(device)\n","\n","    # Load old weights\n","    if load:\n","      try:\n","        net.load_state_dict(torch.load(load)['state_dict'])\n","        print('Model loaded from {}'.format(load))\n","      except Exception as e:\n","        print('The Model could not be loaded, random weights will be used: {}'.format(str(e)))\n","      try:\n","        results_csv = pd.read_csv(dir_results + \"result{}.csv\".format(str(run)))\n","        best_loss = np.min(results_csv[\"validation loss\"])\n","        print('Best loss recorded in last training: {}'.format(best_loss))\n","      except Exception as e:\n","        print(\"Error cargando los datos del csv: {}\".format(str(e)))\n","    else:\n","      best_loss = 10000\n","\n","    train_loader, val_loader = loaders[0], loaders[1]\n","    train_size = len(loaders[0]) * batch_size\n","    loader_size = len(loaders[1]) * batch_size\n","            \n","    # Pretty print of the run\n","    print('''\\n\n","    Starting training:\n","        Dataset: {}\n","        Num Channels: {}\n","        Groundtruth: {}\n","        Num Classes: {}\n","        Folder to save: {}\n","        Load previous: {}\n","        Training size: {}\n","        Validation size: {}\n","        Batch size: {}\n","        Epochs: {}\n","        Learning rate: {}\n","        Optimizer: {}\n","        Loss Function: {}\n","        CUDA: {}\n","    '''.format(dir_img, n_channels, dir_gt, n_classes, dir_results, load, \n","            train_size, loader_size, batch_size, epochs, lr, optimizer, loss, use_cuda))\n","\n","    # Definition of the optimizer ADD MORE IF YOU WANT\n","    if optimizer == \"Adam\":\n","        optimizer = torch.optim.Adam(net.parameters(),\n","                             lr=lr)\n","    elif optimizer == \"SGD\":\n","        optimizer = torch.optim.SGD(net.parameters(),\n","                        lr=lr,\n","                        momentum=0.9,\n","                        weight_decay=0.0005)\n","\n","    # Definition of the loss function ADD MORE IF YOU WANT\n","    if loss == \"MSE\":\n","        criterion = nn.MSELoss()\n","    elif loss == \"MAE\":\n","        criterion = nn.L1Loss()\n","    elif loss == \"CE\":\n","        criterion = CELoss()\n","    elif loss == \"Cross\":\n","        weights = torch.tensor([0.5, 1.0, 2.0])\n","        weights = weights.to(device)\n","        criterion = nn.CrossEntropyLoss(weights)\n","    \n","    # Saving History to csv\n","    header = ['epoch', 'train loss', 'validation loss']\n","    \n","    time_start = time.time()\n","    # Run the training and validation\n","    for epoch in range(epoch_inicial, epochs):\n","        print('\\nStarting epoch {}/{}.'.format(epoch + 1, epochs))\n","\n","        train_loss, train_acc = train_net(net, device, train_loader, optimizer, criterion, batch_size, loss)\n","        val_loss, val_acc = val_net(net, device, val_loader, criterion, batch_size)\n","        \n","        values = [epoch+1, train_loss, val_loss]\n","        export_history(header, values, dir_results, \"result\"+str(run)+\".csv\")\n","\n","        save_checkpoint({\n","                'epoch': epoch + 1,\n","                'state_dict': net.state_dict(),\n","                'loss': train_loss,\n","                'optimizer' : optimizer.state_dict(),\n","              }, path=dir_results, filename='weights' + str(run) + '.pth')\n","        \n","        # save model\n","        if val_loss < best_loss:\n","            best_loss = val_loss\n","            save_checkpoint({\n","                    'epoch': epoch + 1,\n","                    'state_dict': net.state_dict(),\n","                    'loss': train_loss,\n","                    'optimizer' : optimizer.state_dict(),\n","                }, path=dir_results, filename='best_weights' + str(run) + '.pth')\n","\n","    time_dif = time.time() - time_start\n","    print(\"It tooks %.4f seconds to finish the run.\" % (time_dif))\n","    \n","    return train_loss, val_loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BMGqVF558hyO","colab_type":"text"},"source":["# K-Folds Cross Validation\n","As the idea is to make a \"end to end\" K-Fold process over the whole pipeline, we need to make the difference between the proccess for the back and top model. \\\\\n","The main difference being that in the back model we create the partitions while in the top model we should only be using the same partitions. \\\\\n","Therefore, the idea here is to export each partition of ids with each loader in the back model."]},{"cell_type":"markdown","metadata":{"id":"2rarvm04UjoE","colab_type":"text"},"source":["## K-Folds Setup"]},{"cell_type":"code","metadata":{"id":"gk52fSUkloP4","colab_type":"code","colab":{}},"source":["# Function to do a training with Cross Validation for Base Model\n","def Kfolds_validation_single(k, # Amount of K Folds to do\n","                          epoch_inicial,  # Epoch to start the training from - Useful as time in colab is limited\n","                          iteracion_inicial, # Iteration from the k folds to start training from - Same as above\n","                          batch_size, \n","                          dir_img,\n","                          dir_gt, \n","                          dir_predictions, \n","                          dir_results):\n","  \n","  # Obtain the loaders for validation\n","  ids = get_partitions(k)\n","\n","  # Create folders for Back and Top checkpoints\n","  if not os.path.isdir(dir_results):\n","    os.mkdir(dir_results)\n","\n","  for i in range(iteracion_inicial, k):\n","\n","    # We should be loading weights only if the initial epoch is a value over 0\n","    if epoch_inicial != 0:\n","        load = dir_results + \"weights\" + str(i) + \".pth\"\n","    else:\n","        load = None\n","\n","    # Obtains loaders for i-th iteration of kfolds cross validation for back model\n","    back_loaders = get_loaders_kfolds(ids, k, dir_img, dir_gt, batch_size, i, None, None)\n","      \n","    # Runs the training from a certain point in time and for the back model\n","    train_loss, test_loss = setup_and_run_train(n_channels = 1, \n","                                                n_classes = 3, \n","                                                dir_img = dir_img,\n","                                                dir_gt = dir_gt, \n","                                                dir_results = dir_results, \n","                                                load = load, \n","                                                val_perc = 0, \n","                                                batch_size = 10, \n","                                                epoch_inicial = epoch_inicial,\n","                                                epochs = 50,\n","                                                lr = 0.001, \n","                                                run = i, \n","                                                optimizer = \"Adam\", \n","                                                loss = \"CE\", \n","                                                loaders = back_loaders)\n","\n","    # Restarts the epoch_inicial variable so that the next fold we start from the epoch 0\n","    epoch_inicial = 0\n","\n","    print(\"\\nK-Folds {} iteration completed \\n\".format(i))\n","  \n","  print(\"K-Folds training completed\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dAlPzltE8q8O","colab_type":"code","colab":{}},"source":["# Function to do a training with Cross Validation with E2E (End-to-End)\n","def Kfolds_validation_E2E(k, # Amount of K Folds to do\n","                          epoch_inicial,  # Epoch to start the training from - Useful as time in colab is limited\n","                          iteracion_inicial, # Iteration from the k folds to start training from - Same as above\n","                          back, # True if the back model was being trained (If traning needs to start from back) - Same as above\n","                          two_inputs, # Variable to define if training should be done merging the input and DDT prediction\n","                          batch_size, \n","                          dir_img,\n","                          dir_gt, \n","                          dir_gt2, \n","                          dir_predictions, \n","                          dir_results):\n","  \n","  # Obtain the loaders for validation\n","  ids = get_partitions(k)\n","\n","  # When training with \"two inputs\" the amount of channels change to 2 on top_model\n","  if two_inputs:\n","    top_n_channels = 2\n","  else:\n","    top_n_channels = 1\n","\n","  results_back = dir_results + \"Back/\"\n","  results_top = dir_results + \"Top/\"\n","\n","  # Create folders for Back and Top checkpoints\n","  if not os.path.isdir(results_back):\n","    os.makedirs(results_back)\n","  if not os.path.isdir(results_top):\n","    os.makedirs(results_top)\n","\n","  for i in range(iteracion_inicial, k):\n","    # If the back-model needs to be trained\n","    if back:\n","      \n","      # We should be loading weights only if the initial epoch is a value over 0\n","      if epoch_inicial != 0:\n","        load = results_back + \"weights\" + str(i) + \".pth\"\n","      else:\n","        load = None\n","\n","      # Obtains loaders for i-th iteration of kfolds cross validation for back model\n","      back_loaders = get_loaders_kfolds(ids, k, dir_img, dir_gt, batch_size, i, None, None)\n","      \n","      # Runs the training from a certain point in time and for the back model\n","      train_loss, test_loss = setup_and_run_train(n_channels = 1, \n","                                                  n_classes = 1, \n","                                                  dir_img = dir_img,\n","                                                  dir_gt = dir_gt, \n","                                                  dir_results = results_back, \n","                                                  load = load, \n","                                                  val_perc = 0, \n","                                                  batch_size = 10, \n","                                                  epoch_inicial = epoch_inicial,\n","                                                  epochs = 50,\n","                                                  lr = 0.001, \n","                                                  run = i, \n","                                                  optimizer = \"Adam\", \n","                                                  loss = \"MAE\", \n","                                                  loaders = back_loaders)\n","      \n","        \n","      print(\"\\nBack Model training completed\\n\")\n","      \n","      # Resets the epoch_inicial variable so that the top model starts from the epoch 0\n","      epoch_inicial = 0\n","\n","    # We will be running the Top Model always\n","\n","    # We should be loading weights only if the initial epoch is a value over 0\n","    if epoch_inicial != 0:\n","      load = results_top + \"weights\" + str(i) + \".pth\"\n","    else:\n","      load = None\n","    \n","    # Obtains loaders for i-th iteration of kfolds cross validation for top model\n","    back_weights_path = results_back + \"best_weights\" + str(i) + \".pth\"\n","    top_loaders = get_loaders_kfolds(ids, k, dir_img, dir_gt2, batch_size, i, back_weights_path, two_inputs)\n","    \n","    # Runs the training from a certain point in time and for the top model\n","    train_loss, test_loss = setup_and_run_train(n_channels = top_n_channels, \n","                                                n_classes = 3, \n","                                                dir_img = \"Predicted from Back-Model\",\n","                                                dir_gt = dir_gt2, \n","                                                dir_results = results_top, \n","                                                load = load, \n","                                                val_perc = 0, \n","                                                batch_size = 10,\n","                                                epoch_inicial = epoch_inicial,\n","                                                epochs = 50,\n","                                                lr = 0.001, \n","                                                run = i, \n","                                                optimizer = \"Adam\", \n","                                                loss = \"CE\", \n","                                                loaders = top_loaders)\n","    \n","    print(\"\\nTop Model training completed\\n\")\n","\n","    # Restarts the epoch_inicial variable so that the next fold we start from the epoch 0\n","    epoch_inicial = 0\n","\n","    # Regardless, the back model should always be runned on the next iteration\n","    back = True\n","\n","    print(\"\\nK-Folds {} iteration completed \\n\".format(i))\n","  \n","  print(\"K-Folds training completed\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oNJOhMjpUqSH","colab_type":"text"},"source":["## K-Folds End To End Training"]},{"cell_type":"code","metadata":{"id":"vJd5uW-WriDB","colab_type":"code","colab":{}},"source":["#For DNLM03 ONLY\n","#globalMean = 0.1420;\n","#globalStd = 0.1082;\n","\n","Kfolds_validation_E2E(k=5, # Amount of K Folds to do\n","                      epoch_inicial=39,  # Epoch to start the training from - Useful as time in colab is limited\n","                      iteracion_inicial=4, # Iteration from the k folds to start training from - Useful as time in colab is limited\n","                      back=False, # True if the back model was being trained - Useful as time in colab is limited\n","                      two_inputs=False,\n","                      batch_size=10, \n","                      dir_img = 'DNLM03/',\n","                      dir_gt='DTLabels/', \n","                      dir_gt2='BTLabels/', \n","                      dir_predictions=\"predictions/DNLM03/\", \n","                      dir_results='checkpoints/DNLM03/')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ec_ySByt5Kql","colab_type":"text"},"source":["## K-Folds Base Model Training"]},{"cell_type":"code","metadata":{"id":"lEB83LvaVdsP","colab_type":"code","colab":{}},"source":["Kfolds_validation_single(k=0, # Amount of K Folds to do\n","                      epoch_inicial=0,  # Epoch to start the training from - Useful as time in colab is limited\n","                      iteracion_inicial=0, # Iteration from the k folds to start training from - Useful as time in colab is limited\n","                      batch_size=10, \n","                      dir_img = 'Original/',\n","                      dir_gt='BTLabels/', \n","                      dir_predictions=\"predictions/BaseModel/\", \n","                      dir_results='checkpoints/BaseModel/')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q3_TfdG5bLiF","colab_type":"text"},"source":["## K-Folds Prediction"]},{"cell_type":"code","metadata":{"id":"YiuNNoZucr6V","colab_type":"code","colab":{}},"source":["# Saves the dataset of every fold\n","k = 5\n","for i in range(0, k):\n","  save_predictions(i=i,\n","                   k=k,\n","                   dir_predictions=\"predictions/BaseModel/\", \n","                   results_top='checkpoints/BaseModel/', \n","                   results_back=None,\n","                   dir_img='Original/',\n","                   dir_gt='BTLabels/',\n","                   two_inputs=False)\n","  save_predictions(i=i,\n","                   k=k, \n","                   dir_predictions=\"predictions/E2E/\", \n","                   results_top='checkpoints/E2E/Top/', \n","                   results_back='checkpoints/E2E/Back/',\n","                   dir_img='Original/',\n","                   dir_gt='BTLabels/',\n","                   two_inputs=False)\n","  save_predictions(i=i,\n","                   k=k, \n","                   dir_predictions=\"predictions/TwoInputE2E/\", \n","                   results_top='checkpoints/TwoInputE2E/Top/', \n","                   results_back='checkpoints/TwoInputE2E/Back/',\n","                   dir_img='Original/',\n","                   dir_gt='BTLabels/',\n","                   two_inputs=True)\n","  \n","  "],"execution_count":0,"outputs":[]}]}